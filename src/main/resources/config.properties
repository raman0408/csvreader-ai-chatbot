# Only for Ollama
llm.environment=local
ollama.local.baseUrl=http://localhost:11434
ollama.modelName=qwen3:1.7b

# Model behavior
temperature=0.7
timeout.seconds=30
